{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from IPython.core.display import display,Image\n",
    "from string import Template\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import IPython.display\n",
    "import warnings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Defining Convolution layer and its backpropagatoin function \n",
    "\n",
    "#### Back propagation in Conv layer is a full convolution of gradients with output\n",
    "\n",
    "\n",
    "class Conv:\n",
    "    def __init__(self, num_filters, input_shape):\n",
    "        \n",
    "        self.num_filters = num_filters\n",
    "        self.channels = input_shape[2]\n",
    "        self.filters = np.random.randn(self.num_filters,3, 3,self.channels)\n",
    "        self.check = []\n",
    "        \n",
    "        #print(self.channels)\n",
    "    \n",
    "    def conv_operation(self, inp):\n",
    "        \n",
    "#         if len(inp.shape)>2:\n",
    "            \n",
    "#             self.channels = inp.shape[2]\n",
    "#         else:\n",
    "#             self.channels = None\n",
    "        \n",
    "        for i in range(inp.shape[0] -2):\n",
    "            for j in range(inp.shape[1]-2):\n",
    "                \n",
    "                region = inp[i:i+3,j:j+3,:]\n",
    "                #print(region.shape)\n",
    "                \n",
    "                yield i , j , region\n",
    "                \n",
    "    def conv2d(self, input):\n",
    "        \n",
    "        #print(self.filters)\n",
    "        self.last_input = input\n",
    "        \n",
    "        l,w = input.shape[0], input.shape[1]\n",
    "\n",
    "        output = np.zeros((l -2, w-2,self.num_filters))\n",
    "        \n",
    "        for i, j, region in self.conv_operation(input):\n",
    "            \n",
    "            output[i,j] = np.sum(region*self.filters, axis = (1,2,3))\n",
    "            #print(output.shape)\n",
    "            \n",
    "        return output  \n",
    "\n",
    "    def back_prop(self, dl_do, lr=0.1):\n",
    "        \n",
    "        \n",
    "        dl_do_filters = np.zeros(self.filters.shape)\n",
    "        \n",
    "        for i, j ,region in self.conv_operation(self.last_input):\n",
    "            \n",
    "            for f in range(self.num_filters):\n",
    "                #print(region.shape)\n",
    "                \n",
    "                dl_do_filters[f] += dl_do[i,j,f]*region\n",
    "                \n",
    "                #print(region.shape)\n",
    "                \n",
    "        self.filters -= lr*dl_do_filters\n",
    "        \n",
    "        self.check.append(self.filters)\n",
    "        \n",
    "        return None     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### There are no weights for maxpool layer but backpropagation is performed for Maximum activated neuron to \n",
    "#### propagate gradient to the previous layer\n",
    "\n",
    "class MaxPool:\n",
    "    \n",
    "    def pool_operation(self, image):\n",
    "        \n",
    "        new_h = image.shape[0]//2\n",
    "        new_w = image.shape[1]//2\n",
    "        \n",
    "        for i in range(new_h):\n",
    "            for j in range(new_w):\n",
    "                pool_region = image[i*2:i*2+2, j*2:j*2+2]\n",
    "                \n",
    "                yield i , j ,pool_region\n",
    "                \n",
    "    def forward_pool(self, input):\n",
    "        self.last_input = input\n",
    "        \n",
    "        output = np.zeros((input.shape[0]//2, input.shape[1]//2, input.shape[2]))\n",
    "        \n",
    "        for i , j , pool_region in self.pool_operation(input):\n",
    "            \n",
    "            output[i,j] = np.amax(pool_region, axis = (0,1))\n",
    "            \n",
    "        return output\n",
    "    def back_prop(self, dl_do, lr=0.1):\n",
    "        \n",
    "        dl_do_input = np.zeros(self.last_input.shape)\n",
    "        \n",
    "        for i, j , pool_region in self.pool_operation(self.last_input):\n",
    "            \n",
    "            h,w,f = pool_region.shape\n",
    "            amax = np.amax(pool_region, axis = (0,1))\n",
    "            \n",
    "            for ii in range(h):\n",
    "                for jj in range(w):\n",
    "                    for ff in range(f):\n",
    "                        \n",
    "                        if pool_region[ii,jj,ff] == amax[ff]:\n",
    "                            \n",
    "                            dl_do_input[ii*2:ii*2+2, jj*2:jj*2+2, ff] = dl_do[ii,jj,ff]\n",
    "                            break\n",
    "        return dl_do_input    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### Defining softmax and its derivative for backpropagation \n",
    "\n",
    "class Softmax:\n",
    "    \n",
    "    def __init__(self, input_len, nodes):\n",
    "        \n",
    "        self.weights = np.random.randn(input_len, nodes)\n",
    "        self.biases = np.random.randn(nodes)\n",
    "    \n",
    "    \n",
    "    def forward_soft(self, input):\n",
    "        \n",
    "        self.last_input_shape = input.shape\n",
    "        \n",
    "        input = input.flatten()\n",
    "        \n",
    "        self.last_input = input\n",
    "        \n",
    "        input_len, nodes = self.weights.shape\n",
    "        \n",
    "        output = np.dot(input, self.weights)+ self.biases\n",
    "        \n",
    "        self.last_output =  output\n",
    "        \n",
    "        exp = np.exp(output)\n",
    "        \n",
    "        return exp/(np.sum(exp, axis = 0))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def back_prop_soft(self, dl_do, lr=0.1):\n",
    "        \n",
    "        #print(dl_do)\n",
    "        \n",
    "        for i in range(len(dl_do)):\n",
    "            \n",
    "            if dl_do[i] == 0:\n",
    "                continue\n",
    "            exp_out = np.exp(self.last_output)\n",
    "            total_out = np.sum(exp_out)\n",
    "            \n",
    "            #print(exp_out.shape)\n",
    "            do_dt = -exp_out[i]*exp_out/(total_out**2)\n",
    "            do_dt[i] = exp_out[i]*(total_out - exp_out[i])/(total_out**2)\n",
    "            \n",
    "            dt_dw = self.last_input\n",
    "            dt_db = 1\n",
    "            dt_dx = self.weights\n",
    "            \n",
    "            ## dl_dw = dl_do*do_dt*dt_dw\n",
    "            dl_dt = dl_do[i]*do_dt\n",
    "            #print(dl_do.shape)\n",
    "            #print(dl_dt.shape)\n",
    "            dl_dw = dt_dw[np.newaxis].T@dl_dt[np.newaxis]\n",
    "            dl_db = dl_dt*dt_db\n",
    "            dl_dx = dt_dx@dl_dt\n",
    "            \n",
    "            ## update weights and biases\n",
    "#             print(dl_dw.shape)\n",
    "#             print(self.weights.shape)\n",
    "            self.weights += lr*dl_dw\n",
    "            self.biases += lr*dl_db\n",
    "            \n",
    "            return dl_dx.reshape(self.last_input_shape)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
    "\n",
    "train_images = train_X[:1000]\n",
    "train_images = np.array([np.expand_dims(i,-1) for i in train_images])\n",
    "train_labels = train_y[:1000]\n",
    "test_images = test_X[:1000]\n",
    "test_labels = test_y[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = Conv(8,(28,28,1))\n",
    "pool = MaxPool()\n",
    "soft = Softmax(13*13*8,10)\n",
    "\n",
    "\n",
    "def model(image, label):\n",
    "    \n",
    "    conv1 = conv.conv2d(image/255.0-0.5)\n",
    "    po = pool.forward_pool(conv1)\n",
    "    out = soft.forward_soft(po)\n",
    "    \n",
    "    ### Calculate Cross Entropy ####\n",
    "    \n",
    "    loss = -np.log(out[label])\n",
    "    acc = 1 if(np.argmax(out) == label) else 0\n",
    "    \n",
    "    \n",
    "    return out, loss, acc\n",
    "    \n",
    "    \n",
    "\n",
    "def train(image, label, lr = 0.1, epoch = 5):\n",
    "    \n",
    "    \n",
    "    out, loss, acc = model(image, label)\n",
    "    \n",
    "    \n",
    "    #print(out[label])\n",
    "    \n",
    "    gradient = np.zeros(out.shape)\n",
    "    \n",
    "    gradient[label] = -1/out[label]\n",
    "    \n",
    "    ########## Backpropagation########\n",
    "    \n",
    "    \n",
    "    error = soft.back_prop_soft(dl_do = gradient, lr = 0.4)\n",
    "\n",
    "    error = pool.back_prop(error)\n",
    "\n",
    "    error = conv.back_prop(error, lr)\n",
    "    \n",
    "    return loss, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    print('----EPOCH %d ---'%(epoch+1))\n",
    "    \n",
    "    accc = 0\n",
    "    loss = 0\n",
    "    \n",
    "    for i , (image, label) in enumerate(zip(train_images, train_y)):\n",
    "        \n",
    "        if(i>0 and i %100 == 99):\n",
    "            print('[Step %d] Past 100 steps: Average Loss %.3f | Accuracy: %d%%' %(i + 1, loss / 100, accc))\n",
    "            \n",
    "            loss = 0\n",
    "            accc = 0\n",
    "            \n",
    "        l, acc = train(image, label)\n",
    "        loss+= l\n",
    "        acc += accc\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = np.expand_dims(train_X[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = Conv(8, im.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv.check[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(conv.conv2d(im)[:,:,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.random.randn(3,3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[0,0,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
